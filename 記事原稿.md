#タイトル
Watson×VR！Unity初心者でもできる、VR空間でUnityちゃんとおしゃべりアプリ！(v4)

ーーーーーー
今話題の技術や最新技術への挑戦を応援する【IBM×teratailのコラボ企画】第三弾です。今回はWatson×VRを組み合わせたサンプルのご紹介です。

#はじめに

VR元年と言われた2016年以降、「Oculas」や「HTC Vive」、「PlayStation VR」などの様々なVR機器、スマホでも気軽にVRが楽しめる「ハコスコ」などが話題に上がり、また、関東圏では渋谷やお台場にVRが体験できるアミューズメントパークも複数オープンされるなど、VR業界は大きな盛り上がりを見せています。今なおその勢いの止まらないVRの世界、みなさんもVRで何か作ってみたいとワクワクしているかと思いますが、やはりその中でも夢が詰まっているものの一つといえば、VR空間で自分の好きなキャラクターと自由に会話するというものではないでしょうか？

今回はそんな夢のようなVRアプリの第一歩として、Unityちゃんとスマホを通じてVR空間で会話できるようなアプリを作ってみます。

##利用技術について

今回の記事では主にUnity（Versionは5.6.2）を使っていきます。

※Unityのダウンロードは[こちら](https://unity3d.com/jp/get-unity/download)

本記事では、Unityをまったく触ったことがない人でも進められるようになるべく細かく手順を説明していますが、基本操作を確認したい場合は、公式サイトの[チュートリアル](https://unity3d.com/jp/learn/tutorials)を参考にしてください。

Unity内では、C#、JavaScript、Booの3つのプログラミング言語が使えますが、本記事ではC#を用います。C#の基本文法などを一通り確認したい方は、[こちらの記事](http://ufcpp.net/study/csharp/)などが参考になります。

※Unityのドキュメントは[こちら](https://docs.unity3d.com/jp/530/ScriptReference/)
※C#のドキュメントは[こちら](https://docs.microsoft.com/ja-jp/dotnet/csharp/language-reference/)

また、Watson APIも利用するので、Bluemixへのアカウント登録がお済みでない方は[こちら](https://console.ng.bluemix.net/catalog/visual-recognition)より登録を完了させましょう。

#Unityちゃんのデータをゲットしよう

##Unityちゃんとは？

Unityちゃんとは、Unity Technologies Japanが提供する人気オリジナルキャラクターで、開発者がかなり自由に使うことができるよう配慮されています。[ガイドライン](
)や[ライセンス](http://unity-chan.com/contents/license_jp/)、キャラクターの詳しい説明などは[こちら](http://unity-chan.com/)からご確認ください。なお、今回紹介するサンプルは[ユニティちゃんライセンス条項](http://unity-chan.com/contents/license_jp/)の元に提供されています。(© Unity Technologies Japan/UCL)

##Unityちゃんの3Dデータをダウンロード

Unityを立ち上げ、NEW Projectを作成してください。
左上の「Asset Store」タブを選択し、検索窓から「Unity-chan」と入力し検索しましょう。

![1.png](https://qiita-image-store.s3.amazonaws.com/0/99740/abed4913-219a-8d89-81ac-f90d93a377ea.png)

いくつかのUnity-chanに関するアセットが表示されるので、その中から`"Unity-chan!"Model`を選択してください。次のように表示されるので、青い「Import」のボタンをクリックしましょう。

![2.png](https://qiita-image-store.s3.amazonaws.com/0/99740/1622a459-7f09-0de8-e04e-590f079d3be1.png)

次のようなWindowが表示されるので、そのままimportをクリックしてください。

![3.png](https://qiita-image-store.s3.amazonaws.com/0/99740/b87c2cba-e23a-8b37-d43b-3108c7c1dc02.png)

無事にインポートが完了すると、下図のようにProjects内のAssetsフォルダ下にUnityChanというフォルダが作成されます。

![4.png](https://qiita-image-store.s3.amazonaws.com/0/99740/142249ca-3e8d-c724-09f3-405ee7a4e5e3.png)

##Unityちゃんを表示してみよう

`UnityChan->Models`の中に`unitychan`というモデルがみつかるので、それをクリックしてください。次のように右側にinspectorが表示されるので、`Rig`内の`Animation Type`が`Humanoid`になっていることを確認してください。
(もし`Humanoid`になっていなければ、`Humanoid`変更して、`Apply`ボタンを忘れずに押してください。)

![5.png](https://qiita-image-store.s3.amazonaws.com/0/99740/ab9d5f54-e84c-e385-c6fe-9b9ac6b607ee.png)

左上のタブをSceneに切り替え、`unitychan`をHierarchyビューにドラッグ&ドロップしてＵｎｉｔｙちゃんを表示させましょう。

![6.png](https://qiita-image-store.s3.amazonaws.com/0/99740/adb05d1e-c335-8b54-ebae-f1e2798031b7.png)

※シーンビューのアングルの調整方法は、[こちらの公式チュートリアル](https://unity3d.com/jp/learn/tutorials/topics/interface-essentials/scene-view?playlist=45438)を参考にしてください

#Unityちゃんをしゃべらせよう

Unityちゃんをしゃべらせるために、WatsonAPIの「Speach To Text」と「Text To Speach」、「Conversation Service」を使います。話しかけたときの音声データを「Speach To Text」で文字に変換し、「Conversation Service」で解釈し、返ってきた言葉を「Text To Speach」を使って音声で返す、というものです。

##UnitySDKのダウンロード

Watsonには、UnityからWatsonAPIを簡単に呼び出せる「[Unity-SDK](https://github.com/watson-developer-cloud/unity-sdk#ibm-watson-services)」が用意されています。

[こちらのページ](https://github.com/watson-developer-cloud/unity-sdk/releases/tag/0.13.0)より`Sourse code (zip)` ファイルをダウンロードしてください。2017/8/29現在、v0.13.0が最新版なので、本記事ではこのVersionを使います。

ダウンロード->解凍したファイルは、Projectビューの、Assetsファイルの下にドラッグ&ドロップしましょう。ファイル名は、WatsonにRenameしておきます。

Assetの適用が完了すると上部のメニューや、Projectビュー内のAssetsの中のフォルダに「Ｗａｔsｏｎ」というフォルダが追加されます。

![7.png](https://qiita-image-store.s3.amazonaws.com/0/99740/592db6e2-bf88-e954-703a-9b006ca6e3e0.png)

##Speech to textをUnityに適用する

Bluemixにログインし、[ダッシュボード](https://console.bluemix.net/dashboard/apps)を開き、左上のメニューからWatsonを選択しましょう。

![8.png](https://qiita-image-store.s3.amazonaws.com/0/99740/eaa461fa-ffd0-4339-4f43-2c643316028e.png)

カタログの中から、「Speech To Text」を見つけてクリックすると次のように表示されるので、サービス名をSTT-Unity-Chanなど自身が分かりやすいように設定し「作成」ボタンを押してください。

![9.png](https://qiita-image-store.s3.amazonaws.com/0/99740/87d15f6c-3725-fa06-2122-95fe7da9e894.png)

サービスの作成ができたら、次のような画面が表示されるので、左サイドのメニューから「サービス資格情報」を選択してください。

![10.png](https://qiita-image-store.s3.amazonaws.com/0/99740/d502f4ef-b412-9780-61db-98ed9e92f423.png)

続いて、「資格情報の表示」をクリックすると表示されるjson形式のデータが表示されるのでこちらをコピーします。

![11(モザイク必要！).png](https://qiita-image-store.s3.amazonaws.com/0/99740/118e315e-4a56-9cdf-fa1a-53abd1346fcb.png)

Unityの画面に戻ります。先ほど出てきた上部メニューのWatsonをクリックし、Configuration Editorを選択してください。

![12.png](https://qiita-image-store.s3.amazonaws.com/0/99740/fade5793-804a-2900-cdba-6a8b4f7e5868.png)

次のような「Config Editor」のウインドウが開くので、スクロールし、下の方にある「PASTE CREDENTIALS」という欄に先ほどのjsonを貼り付け「ApplyCredentials」をクリックしましょう。

![13.png](https://qiita-image-store.s3.amazonaws.com/0/99740/1922e3ba-0499-e778-2b61-541e55413fd0.png)

Completeと表示されれば完了です。

![14.png](https://qiita-image-store.s3.amazonaws.com/0/99740/971eb5f5-d544-425b-6a00-694365031aed.png)

「Service Speech To Text CONFIGURED」と緑で表示されるようになります。

![15.png](https://qiita-image-store.s3.amazonaws.com/0/99740/2e80d577-54b5-7995-6a42-444964e3a405.png)

##Speach to Textを試してみよう。

公式の[サンプルコード](https://github.com/watson-developer-cloud/unity-sdk#speech-to-text)を参考にしながらSpeach to Textを試してみましょう。

ProjectのAssets->UnityChan->Scriptsのフォルダ直下に、「Create▼」から「C# Script」を選択し、ファイル名を生成します。ファイル名は「SampleSpeachToText」などとしておきます。

![16.png](https://qiita-image-store.s3.amazonaws.com/0/99740/efad3c8d-035e-068b-7972-33ca813f4a4c.png)

SampleSpeachToText.csには、先の[サンプルコード](https://github.com/watson-developer-cloud/unity-sdk#speech-to-text)をベースに、マイクから集音できるように改変します。下記のようになります。

※createでC#ファイルを作成するとき、デフォルトでは「NewBehaviourScript」という名前がついていますが、このまま作成してしまうと、下記コードの`public class SampleSpeachToText : MonoBehaviour { ... }`の部分が`public class NewBehaviourScript : MonoBehaviour { ... }`となってしまいます。ファイル作成後に「NewBehaviourScript」という名前を変更する(該当がいるを右クリック->Renameで変更可能)際は、コード内の「NewBehaviourScript」も書き換えることに注意してください。

```C#
//SampleSpeechToText.cs

using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using IBM.Watson.DeveloperCloud.Services.SpeechToText.v1;

public class SampleSpeechToText : MonoBehaviour {

	[SerializeField]
	private AudioClip m_AudioClip = new AudioClip();
	private SpeechToText m_SpeechToText = new SpeechToText();

	// Use this for initialization
	IEnumerator Start()
	{
		// 音声をマイクから3秒間取得する
		Debug.Log ("Start record"); //集音開始
		var audioSource = GetComponent<AudioSource>();
		audioSource.clip = Microphone.Start(null, true, 10, 44100);
		audioSource.loop = false;
		audioSource.spatialBlend = 0.0f;
		yield return new WaitForSeconds (3f);
		Microphone.End (null); //集音終了
		Debug.Log ("Finish record");

		// ためしに録音内容を再生してみる
		audioSource.Play ();

		// SpeechToTextを日本語指定して、録音音声をテキストに変換
		m_SpeechToText.RecognizeModel = "ja-JP_BroadbandModel";
		m_SpeechToText.Recognize(audioSource.clip, HandleOnRecognize);
	}

	void HandleOnRecognize(SpeechRecognitionEvent result)
	{
		if (result != null && result.results.Length > 0)
		{
			foreach (var res in result.results)
			{
				foreach (var alt in res.alternatives)
				{
					string text = alt.transcript;
					Debug.Log(string.Format("{0} ({1}, {2:0.00})\n", text, res.final ? "Final" : "Interim", alt.confidence));
				}
			}
		}
	}
	
	// Update is called once per frame
	void Update () {
		
	}
}
```

作成したSampleSpeechToText.csをHierarchyビューのunitychanにドラッグ&ドロップしましょう。unitychanをクリックし、そのInspectorを見るとSampleSpeechToTextがComponentに加わっているのが分かります。

![17.png](https://qiita-image-store.s3.amazonaws.com/0/99740/71dd9784-db3e-e68f-4a5b-bebe5b46e844.png)

SampleSpeechToText.csでは、マイクから集音しているので、unitychanにAudioSourceのComponentを付け加える必要があります。unitychan内のInspectorの「Add Component」より、Audio->Audio Sourceをクリックしてください。

![18.png](https://qiita-image-store.s3.amazonaws.com/0/99740/0c69cf86-d6db-e6c0-e2bc-fcc6c8546119.png)

これで準備完了です。Unity画面上部の再生ボタン「▶︎」をクリックして、PCに向けて話しかけてみてください。話しかけた内容がそのまま録音され再生されます。また、Console部分には話しかけた内容が文字で表示されます。

![19.png](https://qiita-image-store.s3.amazonaws.com/0/99740/c732936c-e8d3-95b0-2a05-316024eb5add.png)



##Text to Speachを試してみよう。

先ほどと同様の手順でText to Speachも試して見ましょう。Bluemixの[ダッシュボード](https://console.bluemix.net/dashboard/apps)にログインし、[カタログ](https://console.bluemix.net/catalog/)からText to Speachを選択し、サービス名を「TTS-Unity-Chan」などとして作成します。「資格情報の表示」のjsonファイルをコピーしたら、Unity画面を開き、上部メニューのWatson->Config Editorを開き、「PASTE CREDENTIALS」の欄に貼り付けて「ApplyCredentials」をクリックです。

![20.png](https://qiita-image-store.s3.amazonaws.com/0/99740/7533ca8a-c58b-f091-4a37-685856c58da6.png)

公式の[サンプルコード](https://github.com/watson-developer-cloud/unity-sdk#text-to-speech)を参考にしながらText to Speachを試してみましょう。

ProjectのAssets->UnityChan->Scriptsのフォルダ直下に、「Create▼」から「C# Script」を選択し、ファイルを生成します。ファイル名は「SampleTextToSpeech」としておきます。

SampleTextToSpeech.csのソースは下記となります。

```C#
//SampleTextToSpeech.cs

using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using IBM.Watson.DeveloperCloud.Services.TextToSpeech.v1;

public class SampleTextToSpeech : MonoBehaviour {

	TextToSpeech m_TextToSpeech = new TextToSpeech();
	string m_TestString = "おはようございます。漢字も読めます。";

	// Use this for initialization
	void Start ()
	{
		m_TextToSpeech.Voice = VoiceType.ja_JP_Emi;
		m_TextToSpeech.ToSpeech(m_TestString, HandleToSpeechCallback);
	}

	void HandleToSpeechCallback (AudioClip clip)
	{
		PlayClip(clip);
	}

	private void PlayClip(AudioClip clip)
	{
		if (Application.isPlaying && clip != null)
		{
			GameObject audioObject = new GameObject("AudioObject");
			AudioSource source = audioObject.AddComponent<AudioSource>();
			source.spatialBlend = 0.0f;
			source.loop = false;
			source.clip = clip;
			source.Play();

			GameObject.Destroy(audioObject, clip.length);
		}
	}

	// Update is called once per frame
	void Update () {

	}

}
```

コードが完成したら、先ほどと同様にSampleTextToSpeech.csをHierarchyビュー内のunitychanにドラッグ&ドロップしてください。unitychanのInspectorから、さきほど追加したSampleSpeechToTextのチェックははずしておきます。

![21.png](https://qiita-image-store.s3.amazonaws.com/0/99740/4cb0dd64-b6d4-6b9d-fcdc-cf6b51991c22.png)

再生ボタン「▶︎」をクリックしましょう。「おはようございます。漢字も読めます。」という音声が流ればOKです。動作が確認できたら、unitychanのInspectorから、SampleTextToSpeechのチェックもはずしておきしょう。


##Conversation Serviceを試してみよう。

###Conversation ServiceをUnityに適用させる

さて次はConversation Serviceを試してみます。WatsonのConversation APIでは、こちら側が話しかけた内容に対して、適切な返答を返してくれるよう学習できるものです。

ではConversation APIをUnityに適用させていきましょう。途中までの手順はこれまでと同じで、Bluemixの[ダッシュボード](https://console.bluemix.net/dashboard/apps)にログインし、[カタログ](https://console.bluemix.net/catalog/)からConversationを選択し、サービス名を「Conv-Unity-Chan」などとして作成します。「資格情報の表示」のjsonファイルをコピーしたら、Unity画面を開き、上部メニューのWatson->Config Editorを開き、「PASTE CREDENTIALS」の欄に貼り付けて「ApplyCredentials」をクリックです。

![22.png](https://qiita-image-store.s3.amazonaws.com/0/99740/d93e0d3d-c5b0-a348-a6c1-650ae7840200.png)

###Conversation Serviceの設定

次に、コードを書く前にConversationの設定をしていきます。Conversationを使うには、初期の設定として、どういった問いかけに対して、どのように返答していくかをある程度決める必要があります。それをベースに学習させていき、よりスムーズな会話ができるようになっていくのですが、あまり詳しく説明すると膨大な文量になるため、今回はシンプルにあいさつに対してあいさつを返す程度に止めておこうと思います。

Conversationを設定するにはLaunch Toolというものを使います。自身のBluemixアカウントの[ダッシュボード](https://console.bluemix.net/dashboard)から、先ほど作成した「Conv-Unity-Chan」を選択しましょう。次のような画面となるので、「Launch Tool」をクリックしてください。

![23.png](https://qiita-image-store.s3.amazonaws.com/0/99740/aba4c784-f4ee-5af1-d574-ecadceb5a687.png)

Workspacesに移動するので、createを押しましょう。

![24.png](https://qiita-image-store.s3.amazonaws.com/0/99740/67794a01-e077-9d5d-35c0-482ab7ed5e3c.png)

Nameは「Hello-Unity-Chan」などとし、Descriptionには自身が分かりやすいように記入したあと、createをクリックしてください。

![25.png](https://qiita-image-store.s3.amazonaws.com/0/99740/719e5603-9223-aba1-a1e7-945c82d5fa75.png)

Workspaceを開くと、「Intents」「Entities」「Dialog」のという3つの項目が表示されます。まずはIntentsから設定しましょう。Intentsとは「意図」のことです。ユーザーが、Conversation Serviceに対して話しかけてくるであろう言葉や質問を、その意図ごとに区分けしておく機能がIntentsです。たとえば「あいさつ」を意図として話しかけられるであろう言葉といえば、「おはよう」や「こんにちは」、「こんばんわ」、「やぁ」、「ハロー」などがあるので、これらを「あいさつ」のIntentsとしてまとめて登録します。ではまず、「Create new」をクリックしてください。

![26.png](https://qiita-image-store.s3.amazonaws.com/0/99740/a645072d-aea0-f6ba-7b11-6a13ccb50e4c.png)

「Intent name」には「あいさつ」と入力し、「User example」に思いついたあいさつの言葉をどんどん入れていきます。入れ終えたら「Done」をクリックします。

![27.png](https://qiita-image-store.s3.amazonaws.com/0/99740/9d448229-405a-13ff-2a4e-d8de418b60d5.png)

今回のアプリを完成させるには、これだけでも十分なのですが、後々Conversationでもう少し複雑な会話も設定できるようにもう1個登録してみます。「Intent name」を「食事」として、「User example」に食事をしたいときにConversation Serviceに話しかけそうな言葉を入れてください。

![28.png](https://qiita-image-store.s3.amazonaws.com/0/99740/6ee1d4d1-6c9e-9458-6769-df671a7c90ad.png)

次にEntitiesの設定を行いましょう。EntitiesはEntity(実態)の複数系ですが、「目的語」といったイメージを持っていてください。例えばさきほどの「食事をしたい」と言われたら、何を食べたいかを聞きたくなりますよね？その「何を」にあたるのがEntityです。

ではさっそくEntityを登録してみましょう。左上のメニューを「Entities」に切り替え、「Create new」をクリックします。

![29.png](https://qiita-image-store.s3.amazonaws.com/0/99740/82f34ace-a1f6-a364-acfe-41e3bdcf817f.png)

Entityを「食事メニュー」とし、Valueに思いついた食事メニューを入力していきましょう。記入が完了したら、「Done」をクリックします。

![30.png](https://qiita-image-store.s3.amazonaws.com/0/99740/56dbabaf-9bfd-898a-5ea5-ee1bf14858ca.png)

それでは最後に「Dialog」です。ここで実際の会話のフローを設定していきます。「こんにち」はとあいさつされたらあいさつを返す、「お腹がすいた」と言われたら「何を食べたいですか？」ときき返す...などです。
さっそく設定してみます。左上のメニューを「Dialog」に切り替え、「Create」をクリックします。

![31.png](https://qiita-image-store.s3.amazonaws.com/0/99740/dd810525-fb5a-a28b-bb41-5b78090ac61a.png)

まずは「welcome」というのと「anything_else」というのが表示されます。「welcome」は、一番最初にConversation Serviceが返す言葉(デフォルトは「いらっしゃいませ。ご用件を入力してください。」)が設定されており、「anything_else」では、自分がまだ設定(あるいは学習)していない質問や言葉を投げかけられたときにConversation Serviceが返す言葉(デフォルトでは、「解釈できませんでした。申し訳ありませんが違う表現を試していただけますか。」など)が設定されています。

![32.png](https://qiita-image-store.s3.amazonaws.com/0/99740/2a3a426a-bfed-2bd1-d2d8-72a1de357e70.png)

さて、ここに少し付け加えていきましょう。左上の「Add node」をクリックしてください。すると「welcome」と「anything_else」の間に「No Condition Set」と書かれたnodeが追加され、右にそのnodeの設定画面が表示されます。右部を大まかに説明します。「Name this node...」の部分にはこのnodeの名前を記入します。「If bot recognizes:」の部分には、このnodeが反応する条件を書きます。例えば、ここに「#あいさつ」と記入しておくと、Intentsの「#あいさつ」で設定した「こんにちは」や「おはよう」などの言葉をConversation Serviceが受け取ると反応します。「Enter a response」には、このnodeに反応した際に返す言葉を記入します。

![33.png](https://qiita-image-store.s3.amazonaws.com/0/99740/9f03d7be-9fae-91ec-9782-29f0bbd7ff9e.png)

今回は簡易的に、Intent「#あいさつ」に設定された言葉に対して、「やっほー」と返答するnodeを1つ作成することとします。下記のように記入し、右上の「×」ボタンを押しましょう。また、その上の吹き出しアイコンをクリックすると動作確認ができるので、押してみましょう。

![34.png](https://qiita-image-store.s3.amazonaws.com/0/99740/ad37b913-b6e9-256a-b2eb-6175c5584ea3.png)

右下赤枠で囲んだ部分に「おはよう」などの「#あいさつ」Intentに登録した言葉を入力し、エンターを押してみてくさい。「おはよう」が「#あいさつ」Intentとして認識され、「おはようございます。今日も1日、頑張りましょう！」と返されるので、正常に動作していることが確認できます。

![35.png](https://qiita-image-store.s3.amazonaws.com/0/99740/31f36f5a-e469-c6dc-3f61-356961f12121.png)

なお、「Entities」を活用したり、「child node」を作成し、会話に応じてレスポンスを条件分岐させていくことで、よりインタラクティブな会話が楽しめるようになります。より詳しい使い方が気になる方は、[こちら](https://www.ibm.com/watson/jp-ja/developercloud/conversation.html)のページの動画デモや「[Conversation API 詳細資料を見る](https://www.ibm.com/marketing/iwm/dre/signup?source=mrs-form-9715&S_PKG=ov54914&lang=ja_jp)」ボタンのリンク先で閲覧できる資料などをご参考ください。

###Unity内でConversation Serviceを動かしてみる

まずは今作成したHello-Unity-Chanの「Workspace ID」を確認しましょう。Workspaces画面を開いてください。(先ほどのDialog画面でからだと、左上の「Watson Conversation / Hello-Unity-Chan / Build」の「Watson Conversation」の文字をクリックするとWorkspacesに戻ることができます。)WorkspacesのHello-Unity-Chan内にある縦並びの3つのドットをクリックし、その中から「View details」を選択してください。

![36.png](https://qiita-image-store.s3.amazonaws.com/0/99740/6b996153-40af-3498-ac0a-88c627390630.png)

すると、Workspace IDが表示されるので、こちらをコピーしてください。

![37.png](https://qiita-image-store.s3.amazonaws.com/0/99740/f3fcb517-0cb8-cbb0-53e9-8572a9204646.png)

続けてコードを書いていきます。公式の[サンプルコード](https://github.com/watson-developer-cloud/unity-sdk#conversation)を参考にしましょう。ProjectのAssets->UnityChan->Scriptsのフォルダ直下に、「Create▼」から「C# Script」を選択し、ファイルを生成します。ファイル名は「SampleConversation」などとしておきます。

SampleConversation.csのソースは下記となります。先程コピーしたWorkspace IDは、`private string m_WorkspaceID = "(ここには各々のWorkspace IDが入ります)";`部分に使用してください。

```C#
//SampleConversation.cs

using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using IBM.Watson.DeveloperCloud.Services.Conversation.v1;

public class SampleConversation : MonoBehaviour {

	private Conversation m_Conversation = new Conversation();
	private string m_WorkspaceID = "(ここには各々のWorkspace IDが入ります)"; //各自変更してください
	private string m_Input = "おはよう";

	// Use this for initialization
	void Start () {
		Debug.Log("User: " + m_Input);
		m_Conversation.Message(OnMessage, m_WorkspaceID, m_Input);
	}

	void OnMessage (MessageResponse resp, string customData)
	{
		foreach(Intent mi in resp.intents)
			Debug.Log("intent: " + mi.intent + ", confidence: " + mi.confidence);

		Debug.Log("response: " + resp.output.text.[0]);
	}
	
	// Update is called once per frame
	void Update () {
		
	}
}
```

コードが完成したら、先ほどと同様にSampleTextToSpeech.csをHierarchyビュー内のunitychanにドラッグ&ドロップしてください。そして再生ボタン「▶︎」をクリックしましょう。Console部分に下図のように表示されれば成功です。
※unitychanのInspectorから、さきほど追加したSampleSpeechToTextとSampleTextToSpeechのチェックが外れていることを確認して再生「▶︎」しましょう。

![38.png](https://qiita-image-store.s3.amazonaws.com/0/99740/0195089f-bebc-d59a-6d91-e4095cc837ea.png)

#Unityちゃんと会話ができるようにしよう

さて、今試した3つのWatson API、「Speach to Text」と「Text to Speach」と「Conversation Service」を組み合わせて、「おはよう」と話しかけたら「おはようございます。今日も1日、頑張りましょう！」と返してくれるようにしましょう。
ProjectビューのAssets->UnityChan->Scriptに、「C# Script」を作成してください。ファイル名は「WatsonConversation」としておきましょう。

コードは下記となります。

```C#
//WatsonConversation.cs

using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using IBM.Watson.DeveloperCloud.Services.TextToSpeech.v1;
using IBM.Watson.DeveloperCloud.Services.SpeechToText.v1;
using IBM.Watson.DeveloperCloud.Services.Conversation.v1;

public class WatsonConversation : MonoBehaviour {

	[SerializeField]
	SpeechToText m_SpeechToText = new SpeechToText();
	TextToSpeech m_TextToSpeech = new TextToSpeech();
	private Conversation m_Conversation = new Conversation();
	private string m_WorkspaceID = "(ここには各々のWorkspace IDが入ります)"; //各自変更してください

	// Use this for initialization
	IEnumerator Start() {
		var audioSource = GetComponent<AudioSource>();
		while (true) {
			yield return RecMic(audioSource);
		}
		yield return null;
	}

	IEnumerator RecMic(AudioSource audioSource) {
		Debug.Log ("Start record");
		audioSource.clip = Microphone.Start(null, true, 10, 44100);
		audioSource.loop = false;
		audioSource.spatialBlend = 0.0f;
		yield return new WaitForSeconds (2.0f);
		Microphone.End (null);
		Debug.Log ("Finish record");

		// 音声の認識言語を日本語に指定
		m_SpeechToText.RecognizeModel = "ja-JP_BroadbandModel";
		// 音声をテキストに変換し、関数：HandleOnRecognize()を呼ぶ
		m_SpeechToText.Recognize(audioSource.clip, HandleOnRecognize);

	}

	void HandleOnRecognize(SpeechRecognitionEvent result){
		if (result != null && result.results.Length > 0) {
			foreach (var res in result.results) {
				foreach (var alt in res.alternatives) {
					//集音した音声データを文字化したものをtextに格納
					string text = alt.transcript;
					Debug.Log (string.Format ("{0} ({1}, {2:0.00})\n", text, res.final ? "Final" : "Interim", alt.confidence));

					//textをConversation Serviceに送って処理
					m_Conversation.Message(OnMessage, m_WorkspaceID, text);
				}
			}
		} else {
			//音声ファイルが空あった場合の処理
			Debug.Log ("何も聞き取ってくれないときもある");
		}
	}

	void OnMessage (MessageResponse resp, string customData)
	{
		if (resp != null) {
			foreach (Intent mi in resp.intents) 
				Debug.Log ("intent: " + mi.intent + ", confidence: " + mi.confidence);

			//TextToSpeechの音声タイプを指定
			m_TextToSpeech.Voice = VoiceType.ja_JP_Emi; 
			//Conversation Serviceから返された文字列をTextToSpeechにて発音
			m_TextToSpeech.ToSpeech (resp.output.text[0], HandleToSpeechCallback);

			Debug.Log ("response: " + resp.output.text[0]);
		}
	}

	void HandleToSpeechCallback (AudioClip clip) {
		PlayClip(clip);
	}

	private void PlayClip(AudioClip clip) {
		if (Application.isPlaying && clip != null) {
			GameObject audioObject = new GameObject("AudioObject");
			AudioSource source = audioObject.AddComponent<AudioSource>();
			source.spatialBlend = 0.0f;
			source.loop = false;
			source.clip = clip;
			source.Play();

			GameObject.Destroy(audioObject, clip.length);
		}
	}

	// Update is called once per frame
	void Update () {

	}
}
```

コードが完成したら、WatsonConversation.csをHierarchyビュー内のunitychanにドラッグ&ドロップしましょう。unitychanのInspectorのチェックをWatsonConversation.csだけにして、再生ボタン「▶︎」をクリックしましょう。「おはよう」などと話しかけて、「やっほー」と音声で返ってきたら成功です。(consoleにも表示されます。)

![39.png](https://qiita-image-store.s3.amazonaws.com/0/99740/1c226188-553a-ac47-9d8c-a75fe941ddaa.png)


#Unityちゃんに動きをつけよう

さて「おはよう」と返してくれるようにはなりましたが、これではUnityちゃんが「おはよう」と言ってくれるようには到底思えません。そこでUnityちゃんがちゃんと話してるかのようにするため、おはようと言うタイミングでUnityちゃんに動きをつけてみましょう。

##Unityちゃんのデフォルトの動きを確認しよう。

Unityちゃんにはデフォルトでいくつかの動きが用意されているので、今回はこの動きを活用します。まずはその動きを確認してみましょう。Projects内のAssets->UnityChan->Animationsの中にたくさんのアニメーションが格納されています。その中のどれでもいいのでダブルクリックしてみましょう。右下にモデルを確認する領域が表示されます。

![40.png](https://qiita-image-store.s3.amazonaws.com/0/99740/421c19b2-163e-7e9b-252e-6857b2483097.png)


再生ボタン「▶︎」をクリックすると一応動きは確認できるのですが、このままではアニメーションの動きがわかりづらいのでUnityちゃんに登場してもらいましょう。上図の右下あたりの赤丸で囲んだ人の上半身のようなアイコン(?)をクリック->Othersを選択してください。すると、右上あたりにAsettsが表示されるので、その中からUnitychanを選択しましょう。右下の領域にUnityちゃんが表示されます。

![41.png](https://qiita-image-store.s3.amazonaws.com/0/99740/36860e87-6077-c643-3af6-53cf5c26a16b.png)

あとは、各アニメーションの動きを試してみてください。今回は`unitychan_WATE03`という動きを使います。最後に顔に手をかざすところが、全力で「やっほー！」って言ってるような気がします。デフォルトの状態もただ単に立ってるだけというのも寂しいので、通常の「待ち」状態のときは`unitychan_WATE02`を使って、手をぶらぶらさせておくこととします。

##モーションをつくる

###Stateの作成とStateの遷移の設定
ではさっそくモーションをつくっていきます。今回は単純に、デフォルトの「待ち」の状態(`unitychan_WATE02`で手をぶらぶらさせた状態)->「やっほー！」と発生するときに`unitychan_WATE03`の動きをするというモーションです。

Assets->UnityChan->Animatorsの中に新たにファイルを作成します。create -> Animator Controllerを選択してください。ファイル名はGreetingとしておきます。

![42.png](https://qiita-image-store.s3.amazonaws.com/0/99740/92ae519c-5c17-f443-a0d8-98c13b485840.png)

ファイルを生成したら、そのファイルをダブルクリックしてください。下記のように右上にAnimator画面が表示されます。ここで設定していきます。この一つひとつの箱のことをStateといって、それぞれが3Dモデルの状態を表します。

*「AnyState」は今回は使いません。

![43.png](https://qiita-image-store.s3.amazonaws.com/0/99740/2004f145-73ef-56d1-0e64-43e7adfcfbd9.png)

今回は「待ち」と「あいさつ」の2つの状態(State)を作ります。Animator画面内で右クリックを押し、`Create State->Empty`とを選択してください。「New State」というオレンジの四角が表示されると思います。これをダブルクリックして、Inspopectorを表示、名前を「Wait」に変更してください。

![44.png](https://qiita-image-store.s3.amazonaws.com/0/99740/ea7ce79d-dae5-8e72-3b0c-182375a93717.png)

同様にもう1つStaeteを作成し、名前を「Greet」とします。下図のようになるかと思いますが、続いて状態遷移を設定していきましょう。「Wait」State上で右クリックし、`Make Transition`をクリックしてください。「Wait」から矢印が伸びてくるので、それを「Greet」に繋げましょう。

![45.png](https://qiita-image-store.s3.amazonaws.com/0/99740/a8534abc-baa5-beb8-688c-b2c22a76970a.png)

同様に「Greet」から「Wait」にも矢印を繋げてください。これでStateの作成とStateの遷移の設定は完了です。

![46.png](https://qiita-image-store.s3.amazonaws.com/0/99740/2ce809be-6a37-d22c-4248-c55b4babc0e8.png)

###Stateにアニメーションを設定する

各Stateにアニメーションを紐付けていきます。「Wait」をクリックし、Inspectorを表示させましょう。Motionという項目があるので、その横の「○」をクリックしてください。左上に様々なモーションが表示されるので、「unitychan_WATE02」を選択しましょう。

![47.png](https://qiita-image-store.s3.amazonaws.com/0/99740/d775ae7f-6a95-2ee4-452c-61aa2e2b6944.png)

同様に「Greet」には、「WATE03」を選択してください。

###Parameterの設定

各Stateのへ遷移を管理するために、どのStateなのかを表すパラメータを用意します。左上のParametersをクリック、「+」からBoolを選択しましょう。

![48.png](https://qiita-image-store.s3.amazonaws.com/0/99740/8712a88d-5a54-9954-5d5f-ad5b52a91227.png)

「New Bool」というのが現れるので、名前を「isGreet」に変更しておきます。

![49.png](https://qiita-image-store.s3.amazonaws.com/0/99740/fe68f7a7-e36e-fab4-61a2-076ee30429a2.png)

次に「Wait」から「Greet」に向かっている矢印をクリックしてください。InspectorにConditionsという項目があるかと思いますが、ここの「＋」から「isGreet」が「true」というのを選択してください。これにより、「isGreet」が「true」になると、「Wait」のアニメーション(unitychan_WAIT02)から「Greet」のアニメーション(unitychan_WAIT03)に遷移するという設定が完了します。また、InspectorのSettingsを調整することで、Parameterが変更になったタイミングからどれくらいの時間で遷移先のStateのアニメーションを実行するか、どれくらいの時間をかけて遷移するのかなど設定できるので、調整してみてください。

![50.png](https://qiita-image-store.s3.amazonaws.com/0/99740/de3de4e2-7eb3-bc57-429c-89660acdf1f3.png)

同様の手順で「Greet」から「Wait」に向う矢印のInspectorのConditionsに、「isGreet」の「false」を設定してください。これにより、「isGreet」が「false」になると、「Greet」のアニメーションから「Wait」のアニメーションに遷移します。

###作成したAnimatorを適用する。

Hierarchy内unitychanをクリックしInspectorを表示させましょう。Animatorという項目下にControllerがあるので、右端の「○」をクリックし、今作成した「GoodMorning」を選択しましょう。

![51.png](https://qiita-image-store.s3.amazonaws.com/0/99740/44495854-1c71-1af5-2321-090233ddf154.png)

###スクリプトで状態を切り替えよう

最後の仕上げです。下記のように、ＷａｔｓｏｎＣｏｎｖｅｒｓｔｉｏｎクラス内の初期宣言とHandleOnRecognize()内に一部コードをかき加えましょう。

```C#
//ＷａｔｓｏｎＣｏｎｖｅｒｓｔｉｏｎ.cs
using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using IBM.Watson.DeveloperCloud.Services.TextToSpeech.v1;
using IBM.Watson.DeveloperCloud.Services.SpeechToText.v1;
using IBM.Watson.DeveloperCloud.Services.Conversation.v1;

public class WatsonConversation : MonoBehaviour {

	[SerializeField]
	SpeechToText m_SpeechToText = new SpeechToText();
	TextToSpeech m_TextToSpeech = new TextToSpeech();
	private Conversation m_Conversation = new Conversation();
	private string m_WorkspaceID = "d2490f25-f020-4fda-bf7c-cca59993d8d5";

	private Animator animator; //ここを追加
	private const string key_isGreet = "isGreet"; //ここを追加

	// Use this for initialization
	IEnumerator Start() {
		this.animator = GetComponent<Animator>(); //ここを追加
		var audioSource = GetComponent<AudioSource>();

		while (true) {
			yield return RecMic(audioSource);
		}
		yield return null;
	}

	IEnumerator RecMic(AudioSource audioSource) {
		Debug.Log ("Start record");
		audioSource.clip = Microphone.Start(null, true, 10, 44100);
		audioSource.loop = false;
		audioSource.spatialBlend = 0.0f;
		yield return new WaitForSeconds (2.0f);
		Microphone.End (null);
		Debug.Log ("Finish record");

		// 音声の認識言語を日本語に指定
		m_SpeechToText.RecognizeModel = "ja-JP_BroadbandModel";
		// 音声をテキストに変換し、関数：HandleOnRecognize()を呼ぶ
		m_SpeechToText.Recognize(audioSource.clip, HandleOnRecognize);

	}

	void HandleOnRecognize(SpeechRecognitionEvent result){
		if (result != null && result.results.Length > 0) {
			foreach (var res in result.results) {
				foreach (var alt in res.alternatives) {
					//集音した音声データを文字化したものをtextに格納
					string text = alt.transcript;
					Debug.Log (string.Format ("{0} ({1}, {2:0.00})\n", text, res.final ? "Final" : "Interim", alt.confidence));

					//textをConversation Serviceに送って処理
					m_Conversation.Message(OnMessage, m_WorkspaceID, text);
				}
			}
		} else {
			//音声ファイルが空あった場合の処理
			Debug.Log ("何も聞き取ってくれないときもある");
		}
	}

	void OnMessage (MessageResponse resp, string customData)
	{
		if (resp != null) {
			foreach (Intent mi in resp.intents) 
				Debug.Log ("intent: " + mi.intent + ", confidence: " + mi.confidence);

			if (resp.output.text [0].Contains ("やっほー")) {
				this.animator.SetBool (key_isGreet, true);
			}

			//TextToSpeechの音声タイプを指定
			m_TextToSpeech.Voice = VoiceType.ja_JP_Emi; 
			//Conversation Serviceから返された文字列をTextToSpeechにて発音
			m_TextToSpeech.ToSpeech (resp.output.text[0], HandleToSpeechCallback);

			Debug.Log ("response: " + resp.output.text[0]);
		}
	}

	void HandleToSpeechCallback (AudioClip clip) {
		PlayClip(clip);
	}

	private void PlayClip(AudioClip clip) {
		if (Application.isPlaying && clip != null) {
			GameObject audioObject = new GameObject("AudioObject");
			AudioSource source = audioObject.AddComponent<AudioSource>();
			source.spatialBlend = 0.0f;
			source.loop = false;
			source.clip = clip;
			source.Play();

			GameObject.Destroy(audioObject, clip.length);
		}
	}

	// Update is called once per frame
	void Update () {

	}
}
```

完了したら、再生ボタンを押して動作確認をしましょう。「おはよう」などと声をかけた際に、音声だけでなく、動きも確認できたら成功です。

![52.5WatsonVR.gif](https://qiita-image-store.s3.amazonaws.com/0/99740/415597c1-adad-6126-9935-fbbf4d1e923e.gif)

※声と動作の基本最初はタイミングがズレるので、細かいチューニングが必要になってきます。

#VR対応にしよう

最後に、スマホで見る場合を想定して、スマホのジャイロセンサーに反応するようにしましょうProjectのUnityChan -> Scriptsの中に、create -> C# Scriptで、gyro.csを作成してください。そして下記のように付け加えます。

```C#
//gyro.cs

using System.Collections;
using System.Collections.Generic;
using UnityEngine;

public class gyro : MonoBehaviour {

	// Use this for initialization
	void Start () {
		Input.gyro.enabled = true;
	}
	
	// Update is called once per frame
	void Update () {
		transform.rotation = Quaternion.AngleAxis(90.0f,Vector3.right)*Input.gyro.attitude*Quaternion.AngleAxis(180.0f,Vector3.forward);
	}
}
```

#実機で確認しよう

左上メニューのFile->Build Settings...を選択してください。

![53.png](https://qiita-image-store.s3.amazonaws.com/0/99740/0503e330-8307-fc0a-4a88-6d8a67c7a702.png)

下記のようなウィンドウが開かれるので、ご自身がお使いの実機を選択し、「Build And Run」を選択しましょう。

![54.png](https://qiita-image-store.s3.amazonaws.com/0/99740/3da1b82c-c856-c202-a120-23a3dac67789.png)

なお、実機でのBuildについては、実機側の設定や、iOSだとDeveloper登録等も必要になってくるため、ここでは詳細は割愛させていただきますが、参考サイトして、下記サイトをご覧ください。

#おわりに
いかがでしたでしょうか？
今回はUnityを使って簡単に3Dキャラクターの設定手順と、「Speach To Text」「Text To Speach」「Conversation Service」の組み込み方をご紹介しました。ここで紹介した方法で、3Dモデルの動きを強化したり、Conversationの会話パターンをさらにつくり込むことで、自分の理想の3Dモデルへと成長させていくことができるかと思います。ぜひいろいろ試してみてください。

